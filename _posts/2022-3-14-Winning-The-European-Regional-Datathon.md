---
excerpt: How me and a few friends did a datathon and won it

---

{% include head.html %}
# Winning Correlation One & Citadel's 2022 Spring European Datathon

This is a brief account to how my team won the Citadel 2022 Spring European Datathon.

### End Product

The goal of the dataset was to write an open-ended report on a supplied data set analysing its characteristics. Teams of 4 compete to make the best report they can in a week. You can see our report [here](https://anonymous3141.github.io/static/Team_6_Report.pdf)
### Team Formation

A few weeks ago, I saw this event via a Cambridge notice bulletin, an Datathon jointly hosted by Correlation One and Citadel (a big quant fund). Having done some data science in the past year but never too comfortable with it, I decided to participate to help alleviate my fear of pandas dataframes. 

With some effort, I had scratched together a team of fellow Cambridge first year friends to do the contest with me. There was tunan, my Australian compatriot and a computer science student, Sherman, fellow trinity maths student and data science veteran who had won the last iteration of the datathon, and Wilson, an maths student from Emmanuel College. We passed the entrance test to the datathon despite failing some questions that first years have no hope of doing, and thus successfully formed our team. 

In reality, we were just in it for the fun and experience. As first years, we didn't expect to be able to outshine  the 15 other teams, containing 2nd and 3rd (maybe even Masters) students from the top universities in the UK.

### Competition Schedule
The task was announced on Sunday the 6th March, concerning the Supply Chain of an international Company in the form of a list of orders and customers. The data was released the morning of the 7th, and our team of 4 would have a full week to write an open-ended report (there were some suggested prompts but we could do anything we thought was interesting) on the data, and submit before 10pm the following Sunday (13th), and the results would be announced on Pi Day the 14th.


### How we went about it

Everyday for the week, the team would meet at Trinity to work for some hours on the project. Initially it was just about brainstorming avenues of exploration, and using exploratory data analysis (plotting lots of stuff, eyeballing series) to see if our questions were promising to explore. 

The data exhibited surprising uniformity and it was initially hard to find any interesting behaviour within the dataset. However, as the week went on, nontrivial trends and relations emerged.

By the middle of the week we had to start choosing directions to dig into, and to start constructing models as well as running hypothesis tests to give the necessary depth and rigour to our research. 

By Saturday, we had to start wrapping up our analysis and start writing the report, as well as collating and arranging data for presentation. The report was extremely long to write up, and took the entirety of Sunday to write and to convert into latex format. When we submitted with 20 minutes to spare (about 9:40pm, our group had collaboratively worked since the morning to write and format the report),  our report spanned 9 pages and 5000 words.

### The Teamwork

We were extremely proud of the product we made when we submitted. This was partially due to the division of labour being especially effective, as every person was able to demonstrate their technical strengths (for instance, Tunan's speed-latexing skills and Sherman's knowledge of statistics).  

I focused on exploratory data analysis and the machine learning model fitting, was able to learn a lot from the others, with my abilities in Latex, Pandas and Matplotlib now much improved.

### Announcing the winners

The winners were announced just after 7pm on Monday the 14th. While we had not expected to win, we were very hopeful that we'd get at least a podium place. 

We were very surprised, but nevertheless very pleased to hear that we had won. I think it ultimately came down to our more meticulous data engineering. It seemed many teams forgot to remove inputs to their machine learning models that could have encoded the answer, and resulted in models that were extremely accurate but which was the result of secretly feeding the answer in. In particular, we were more suspicious when things seemed "too good to be true" (see report section VIII) and scrutinised our data more carefully.

### Concluding remarks

We are grateful for Citadel's organisation of the datathon, it was a valuable learning opportunity to our team, and winning was the just cherry on the cake.
